{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, UpSampling2D\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\tensorflow\\__init__.py:462\u001b[0m\n\u001b[0;32m    460\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_keras.src.optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    461\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 462\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.src.optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[0;32m    464\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# DO NOT EDIT. Generated by api_gen.sh\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTypePolicy\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatDTypePolicy\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\api\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\api\\activations\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\activations\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tanh\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_registration\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n\u001b[0;32m     25\u001b[0m ALL_OBJECTS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     26\u001b[0m     relu,\n\u001b[0;32m     27\u001b[0m     leaky_relu,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     log_softmax,\n\u001b[0;32m     44\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\saving\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_registered_object\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_keras_serializable\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_object\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize_keras_object\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m legacy_h5_format\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_utils\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m json_utils\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_options\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_utils\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_registration\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m metrics_module\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\models\\functional.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_utils\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization \u001b[38;5;28;01mas\u001b[39;00m legacy_serialization\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _build_map\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\models\\model.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariable_mapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_saveable_variables\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_api\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trainer \u001b[38;5;28;01mas\u001b[39;00m base_trainer\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary_utils\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceback_utils\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileLoss\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileMetrics\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter_utils\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traceback_utils\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tracking\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribution_lib\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_data_adapter\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_dataset_adapter\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_data_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrayDataAdapter\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_slicing\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter_utils\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataAdapter\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_adapters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter_utils\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\pandas\\__init__.py:36\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401,E501\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\pandas\\compat\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     pa_version_under7p0,\n\u001b[0;32m     29\u001b[0m     pa_version_under8p0,\n\u001b[0;32m     30\u001b[0m     pa_version_under9p0,\n\u001b[0;32m     31\u001b[0m     pa_version_under11p0,\n\u001b[0;32m     32\u001b[0m     pa_version_under13p0,\n\u001b[0;32m     33\u001b[0m     pa_version_under14p0,\n\u001b[0;32m     34\u001b[0m     pa_version_under14p1,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\pandas\\compat\\pyarrow.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(Version(pa\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version)\n\u001b[0;32m     11\u001b[0m     pa_version_under7p0 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\2024\\Lib\\site-packages\\pyarrow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_generated_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m __version__\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Package is not installed, parse git tag at runtime\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1616\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1659\u001b[0m, in \u001b[0;36m_fill_cache\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import shutil\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Activation, Add\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "train_limit = 6\n",
    "val_limit = 2\n",
    "test_limit = 1\n",
    "max_visualizations = 5  # Limite d'exemples à afficher\n",
    "\n",
    "input_dir_rgb = 'P8_Cityscapes_leftImg8bit_trainvaltest'\n",
    "input_dir_masks = 'P8_Cityscapes_gtFine_trainvaltest'\n",
    "output_dir = 'processed_data'\n",
    "\n",
    "# Créer les dossiers de sortie\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'train', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'train', 'masks'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'val', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'val', 'masks'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'test', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'test', 'masks'), exist_ok=True)\n",
    "\n",
    "# Mapping des classes `labelIds` vers les 8 classes cibles\n",
    "label_mapping = {\n",
    "    # Flat\n",
    "    7: 0,  # road\n",
    "    8: 0,  # sidewalk\n",
    "    \n",
    "    # Human\n",
    "    24: 1,  # person\n",
    "    25: 1,  # rider\n",
    "    \n",
    "    # Vehicle\n",
    "    26: 2,  # car\n",
    "    27: 2,  # truck\n",
    "    28: 2,  # bus\n",
    "    31: 2,  # train\n",
    "    32: 2,  # motorcycle\n",
    "    33: 2,  # bicycle\n",
    "    \n",
    "    # Construction\n",
    "    11: 3,  # building\n",
    "    12: 3,  # wall\n",
    "    13: 3,  # fence\n",
    "    \n",
    "    # Object\n",
    "    17: 4,  # pole\n",
    "    18: 4,  # traffic light\n",
    "    19: 4,  # traffic sign\n",
    "    \n",
    "    # Nature\n",
    "    21: 5,  # vegetation\n",
    "    22: 5,  # terrain\n",
    "    \n",
    "    # Sky\n",
    "    23: 6,  # sky\n",
    "    \n",
    "    # Void\n",
    "    0: 7,  # void\n",
    "    1: 7,  # ego vehicle\n",
    "    2: 7,  # rectification border\n",
    "    3: 7,  # out of roi\n",
    "    4: 7,  # static\n",
    "    5: 7,  # dynamic\n",
    "    6: 7,  # ground\n",
    "    9: 7,  # parking\n",
    "    10: 7,  # rail track\n",
    "    14: 7,  # guard rail\n",
    "    15: 7,  # bridge\n",
    "    16: 7,  # tunnel\n",
    "    20: 7,  # polegroup\n",
    "    29: 7,  # caravan\n",
    "    30: 7,  # trailer\n",
    "    -1: 7   # Ignore\n",
    "}\n",
    "\n",
    "def remap_labels(mask, mapping):\n",
    "    remapped_mask = np.copy(mask)\n",
    "    for original_value, new_value in mapping.items():\n",
    "        remapped_mask[mask == original_value] = new_value\n",
    "    remapped_mask[~np.isin(mask, list(mapping.keys()))] = 7  # Assurez-vous que les autres valeurs soient Void\n",
    "    return remapped_mask\n",
    "\n",
    "def process_city(city_dir, mask_dir, output_subdir, visualize=False):\n",
    "    city_name = os.path.basename(city_dir)\n",
    "    rgb_images = sorted(os.listdir(city_dir))\n",
    "    \n",
    "    for img_file in rgb_images:\n",
    "        img_id = img_file.replace('_leftImg8bit.png', '')\n",
    "        \n",
    "        # Charger l'image RGB\n",
    "        rgb_path = os.path.join(city_dir, img_file)\n",
    "        rgb_image = Image.open(rgb_path)\n",
    "        \n",
    "        # Charger le masque `labelIds`\n",
    "        mask_label_path = os.path.join(mask_dir, f\"{img_id}_gtFine_labelIds.png\")\n",
    "        mask_label = np.array(Image.open(mask_label_path))\n",
    "        \n",
    "        # Remapper les labels du masque\n",
    "        mask_label = remap_labels(mask_label, label_mapping)\n",
    "\n",
    "        # Sauvegarder les images et les masques\n",
    "        rgb_image.save(os.path.join(output_dir, output_subdir, 'images', f\"{img_id}_image.png\"))\n",
    "        Image.fromarray(mask_label).save(os.path.join(output_dir, output_subdir, 'masks', f\"{img_id}_mask.png\"))\n",
    "\n",
    "# Fonction pour traiter un ensemble d'images\n",
    "def process_dataset(subset_name, limit, visualize=False):\n",
    "    subset_rgb_dir = os.path.join(input_dir_rgb, subset_name)\n",
    "    subset_mask_dir = os.path.join(input_dir_masks, subset_name)\n",
    "    cities = sorted(os.listdir(subset_rgb_dir))[:limit]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for city in cities:\n",
    "            city_rgb_dir = os.path.join(subset_rgb_dir, city)\n",
    "            city_mask_dir = os.path.join(subset_mask_dir, city)\n",
    "            executor.submit(process_city, city_rgb_dir, city_mask_dir, subset_name, visualize)\n",
    "\n",
    "# Traiter les ensembles train, val, et test avec visualisation pour validation\n",
    "process_dataset('train', train_limit, visualize=True)\n",
    "process_dataset('val', val_limit, visualize=True)\n",
    "process_dataset('test', test_limit, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données Sans Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class DataGenerator\n",
    "class DataGenerator:\n",
    "    def __init__(self, image_files, label_files, batch_size=4, image_size=(256, 256), n_classes=8, augment=False, limit=None):\n",
    "        if limit:\n",
    "            self.image_files = image_files[:limit]\n",
    "            self.label_files = label_files[:limit]\n",
    "        else:\n",
    "            self.image_files = image_files\n",
    "            self.label_files = label_files\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.n_classes = n_classes\n",
    "        self.augment = augment\n",
    "\n",
    "    def load_and_preprocess_image(self, image_path, label_path):\n",
    "        try:\n",
    "            image = img_to_array(load_img(image_path, target_size=self.image_size)) / 255.0\n",
    "            mask = np.array(load_img(label_path, target_size=self.image_size, color_mode=\"grayscale\"))\n",
    "            mask = np.eye(self.n_classes)[mask]\n",
    "            return image.astype('float32'), mask.astype('float32')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {image_path} or {label_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def create_dataset(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.image_files, self.label_files))\n",
    "        dataset = dataset.map(lambda img, lbl: tf.numpy_function(self.load_and_preprocess_image, [img, lbl], [tf.float32, tf.float32]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.map(lambda img, mask: (tf.ensure_shape(img, [self.image_size[0], self.image_size[1], 3]), \n",
    "                                                 tf.ensure_shape(mask, [self.image_size[0], self.image_size[1], self.n_classes])), \n",
    "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        if self.augment:\n",
    "            dataset = dataset.map(lambda img, mask: self.augment_image(img, mask), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.batch(self.batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les chemins vers les dossiers de données prétraitées\n",
    "data_dir = 'C:/Users/DELL/Desktop/OpenClass/Formation/Projet_008/processed_data'\n",
    "\n",
    "# Charger les chemins des images et des masques pour l'entraînement et la validation\n",
    "train_image_files = [os.path.join(data_dir, 'train', 'images', f) for f in sorted(os.listdir(os.path.join(data_dir, 'train', 'images'))) if f.endswith('.png')]\n",
    "train_label_files = [os.path.join(data_dir, 'train', 'masks', f) for f in sorted(os.listdir(os.path.join(data_dir, 'train', 'masks'))) if f.endswith('.png')]\n",
    "\n",
    "val_image_files = [os.path.join(data_dir, 'val', 'images', f) for f in sorted(os.listdir(os.path.join(data_dir, 'val', 'images'))) if f.endswith('.png')]\n",
    "val_label_files = [os.path.join(data_dir, 'val', 'masks', f) for f in sorted(os.listdir(os.path.join(data_dir, 'val', 'masks'))) if f.endswith('.png')]\n",
    "\n",
    "# Définir la taille de l'ensemble de données pour les tests\n",
    "data_limit = 20 \n",
    "\n",
    "# Créer les datasets avec DataGenerator\n",
    "train_generator = DataGenerator(train_image_files, train_label_files, batch_size=2, image_size=(256, 256), n_classes=8, augment=False, limit=100).create_dataset()\n",
    "train_generator = train_generator.take(20)\n",
    "val_generator = DataGenerator(val_image_files, val_label_files, batch_size=2, image_size=(256, 256), n_classes=8, augment=False, limit=data_limit).create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer le Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)\n",
    "\n",
    "# Fonction pour calculer l'IoU corrigé\n",
    "def iou(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1, 2, 3])\n",
    "    union = K.sum(y_true, axis=[1, 2, 3]) + \\\n",
    "        K.sum(y_pred, axis=[1, 2, 3]) - intersection\n",
    "    return K.mean((intersection + 1e-6) / (union + 1e-6), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Unet_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_mini(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(0.5)(pool1)\n",
    "\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = Dropout(0.5)(conv3)\n",
    "\n",
    "    up4 = UpSampling2D(size=(2, 2))(pool3)\n",
    "    merge4 = concatenate([conv2, up4], axis=3)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(merge4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "\n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    merge5 = concatenate([conv1, up5], axis=3)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(merge5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(8, 1, activation='softmax')(conv5)  # 8 classes\n",
    "\n",
    "    model = Model(inputs, conv5)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss=dice_loss, metrics=[iou])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_mini(learning_rate, image_files_train, label_files_train, image_files_val, label_files_val, image_files_test, label_files_test):\n",
    "    batch_size = 4\n",
    "    print(f\"Training with learning rate: {learning_rate}\")\n",
    "    \n",
    "    train_gen = DataGenerator(image_files_train, label_files_train, batch_size=batch_size).create_dataset()\n",
    "    val_gen = DataGenerator(image_files_val, label_files_val, batch_size=batch_size).create_dataset()\n",
    "    test_gen = DataGenerator(image_files_test, label_files_test, batch_size=batch_size).create_dataset()\n",
    "\n",
    "    model = unet_mini()\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=dice_loss, metrics=[iou])\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    \n",
    "    checkpoint_dir = f\"lr_{learning_rate}\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.keras\")\n",
    "    model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=35,\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, reduce_lr, early_stopping]  # Ajout d'EarlyStopping ici\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    with open(f'history_lr_{learning_rate}.pkl', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "        \n",
    "    train_iou = history.history['iou'][-1]\n",
    "    val_iou = history.history['val_iou'][-1]\n",
    "    \n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    loss, test_iou = model.evaluate(test_gen, verbose=1)\n",
    "    \n",
    "    print(f\"Training Time for learning rate {learning_rate}: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Training IoU for learning rate {learning_rate}: {train_iou:.2f}\")\n",
    "    print(f\"Val IoU for learning rate {learning_rate}: {val_iou:.2f}\")\n",
    "    print(f\"Test IoU for learning rate {learning_rate}: {test_iou:.2f}\")\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save(f\"mini_unet_LR_{learning_rate}.keras\")\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    # clear memory\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "learning_rates = [1e-4]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    train_mini(\n",
    "        learning_rate=lr,\n",
    "        image_files_train=train_image_files,\n",
    "        label_files_train=train_label_files,\n",
    "        image_files_val=val_image_files,\n",
    "        label_files_val=val_label_files,\n",
    "        image_files_test=val_image_files,  # Utiliser les fichiers de validation comme test\n",
    "        label_files_test=val_label_files\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle sauvegardé\n",
    "model_path = 'mini_unet2_LR_0.0001.keras'  # Remplacez par le chemin de votre modèle\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_loss': dice_loss, 'iou': iou})\n",
    "\n",
    "def create_jet_palette(num_classes=8):\n",
    "    cmap = plt.get_cmap('jet', num_classes)\n",
    "    palette = {i: np.array(cmap(i)[:3]) * 255 for i in range(num_classes)}\n",
    "    return palette\n",
    "\n",
    "def color_mask(mask, palette):\n",
    "    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for label, color in palette.items():\n",
    "        color_mask[mask == label] = color\n",
    "    return color_mask\n",
    "\n",
    "# Créer la palette jet pour 8 classes\n",
    "palette = create_jet_palette(num_classes=8)\n",
    "\n",
    "# Choisir une image de validation spécifique\n",
    "image_index = 0  # Choisissez l'indice de l'image\n",
    "\n",
    "# Charger l'image d'entrée et le masque de référence (labelIds.png)\n",
    "input_image, true_mask = next(iter(val_generator))\n",
    "input_image = input_image.numpy()[image_index]  # Récupérer l'image en tant que NumPy array\n",
    "input_image_rescaled = (input_image * 255).astype(np.uint8)  # Rescale les valeurs pour être dans [0, 255]\n",
    "true_mask = true_mask[image_index]\n",
    "\n",
    "# Colorer le masque de référence\n",
    "true_mask_resized = np.argmax(true_mask, axis=-1)\n",
    "true_mask_color = color_mask(true_mask_resized, palette)\n",
    "\n",
    "# Effectuer la prédiction\n",
    "predicted_mask = model.predict(np.expand_dims(input_image, axis=0))[0]\n",
    "predicted_mask_resized = np.argmax(predicted_mask, axis=-1)\n",
    "\n",
    "# Colorer la prédiction pour la comparer\n",
    "colored_predicted_mask = color_mask(predicted_mask_resized, palette)\n",
    "\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(input_image_rescaled)\n",
    "plt.title('Input Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(true_mask_color)\n",
    "plt.title('True Mask (labelIds converted to color)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(colored_predicted_mask)\n",
    "plt.title('Predicted Mask')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model DeepLabV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger DeepLabV3 avec un backbone ResNet50\n",
    "def deeplabv3_model(input_size=(256, 256, 3), n_classes=8):\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet', \n",
    "        include_top=False, \n",
    "        input_tensor=Input(input_size)\n",
    "    )\n",
    "    \n",
    "    # Sortie du backbone\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Remise à l'échelle de la sortie pour correspondre à la taille de l'entrée\n",
    "    x = UpSampling2D(size=(32, 32))(x) \n",
    "    \n",
    "    # Ajouter un segment de sortie personnalisé\n",
    "    x = tf.keras.layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss=dice_loss, metrics=[iou])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Modifiez votre fonction de formation pour utiliser le modèle DeepLabV3\n",
    "def train_mini(learning_rate, image_files_train, label_files_train, image_files_val, label_files_val, image_files_test, label_files_test):\n",
    "    batch_size = 4\n",
    "    print(f\"Training with learning rate: {learning_rate}\")\n",
    "    \n",
    "    train_gen = DataGenerator(image_files_train, label_files_train, batch_size=batch_size).create_dataset()\n",
    "    val_gen = DataGenerator(image_files_val, label_files_val, batch_size=batch_size).create_dataset()\n",
    "    test_gen = DataGenerator(image_files_test, label_files_test, batch_size=batch_size).create_dataset()\n",
    "\n",
    "    model = deeplabv3_model()  # Utilisation de DeepLabV3\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=dice_loss, metrics=[iou])\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    checkpoint_dir = f\"lr_{learning_rate}\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.keras\")\n",
    "    model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=35,\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, reduce_lr, early_stopping]  # Ajout de EarlyStopping\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Sauvegarde du fichier history sans \"DataAug\"\n",
    "    with open(f'history_lr_{learning_rate}.pkl', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "        \n",
    "    train_iou = history.history['iou'][-1]\n",
    "    val_iou = history.history['val_iou'][-1]\n",
    "    \n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    loss, test_iou = model.evaluate(test_gen, verbose=1)\n",
    "    \n",
    "    print(f\"Training Time for learning rate {learning_rate}: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Training IoU for learning rate {learning_rate}: {train_iou:.2f}\")\n",
    "    print(f\"Val IoU for learning rate {learning_rate}: {val_iou:.2f}\")\n",
    "    print(f\"Test IoU for learning rate {learning_rate}: {test_iou:.2f}\")\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save(f\"deeplabv3_LR_{learning_rate}.keras\")\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    # clear memory\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "learning_rates = [1e-4]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    train_mini(\n",
    "        learning_rate=lr,\n",
    "        image_files_train=train_image_files,\n",
    "        label_files_train=train_label_files,\n",
    "        image_files_val=val_image_files,\n",
    "        label_files_val=val_label_files,\n",
    "        image_files_test=val_image_files,\n",
    "        label_files_test=val_label_files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger l'historique de l'entraînement à partir du fichier pickle sauvegardé\n",
    "with open(f'history_lr_0.0001.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "# Récupérer les données d'IoU et de Dice (Dice est calculé à partir de l'IoU)\n",
    "epochs = range(1, len(history['iou']) + 1)\n",
    "iou_train = history['iou']\n",
    "iou_val = history['val_iou']\n",
    "\n",
    "# Calculer le coefficient Dice à partir de l'IoU\n",
    "dice_train = [2 * iou / (1 + iou) for iou in iou_train]\n",
    "dice_val = [2 * iou / (1 + iou) for iou in iou_val]\n",
    "\n",
    "# Graphique d'évolution de l'IoU\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, iou_train, label='IoU Entraînement', marker='o')\n",
    "plt.plot(epochs, iou_val, label='IoU Validation', marker='o')\n",
    "plt.title('Évolution de l\\'IoU au fil des epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Graphique d'évolution du Dice coefficient\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, dice_train, label='Dice Entraînement', marker='o')\n",
    "plt.plot(epochs, dice_val, label='Dice Validation', marker='o')\n",
    "plt.title('Évolution du Dice coefficient au fil des epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model_path = 'deeplabv3_LR_0.0001.keras'  # Remplacez par le chemin de votre modèle\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_loss': dice_loss, 'iou': iou})\n",
    "\n",
    "def create_jet_palette(num_classes=8):\n",
    "    cmap = plt.get_cmap('jet', num_classes)\n",
    "    palette = {i: np.array(cmap(i)[:3]) * 255 for i in range(num_classes)}\n",
    "    return palette\n",
    "\n",
    "def color_mask(mask, palette):\n",
    "    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for label, color in palette.items():\n",
    "        color_mask[mask == label] = color\n",
    "    return color_mask\n",
    "\n",
    "# Créer la palette jet pour 8 classes\n",
    "palette = create_jet_palette(num_classes=8)\n",
    "\n",
    "# Choisir une image de validation spécifique\n",
    "image_index = 0  # Choisissez l'indice de l'image\n",
    "\n",
    "# Charger l'image d'entrée et le masque de référence (labelIds.png)\n",
    "input_image, true_mask = next(iter(val_generator))\n",
    "input_image = input_image.numpy()[image_index]  # Récupérer l'image en tant que NumPy array\n",
    "input_image_rescaled = (input_image * 255).astype(np.uint8)  # Rescale les valeurs pour être dans [0, 255]\n",
    "true_mask = true_mask[image_index]\n",
    "\n",
    "# Colorer le masque de référence\n",
    "true_mask_resized = np.argmax(true_mask, axis=-1)\n",
    "true_mask_color = color_mask(true_mask_resized, palette)\n",
    "\n",
    "# Effectuer la prédiction\n",
    "predicted_mask = model.predict(np.expand_dims(input_image, axis=0))[0]\n",
    "predicted_mask_resized = np.argmax(predicted_mask, axis=-1)\n",
    "\n",
    "# Colorer la prédiction pour la comparer\n",
    "colored_predicted_mask = color_mask(predicted_mask_resized, palette)\n",
    "\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(input_image_rescaled)\n",
    "plt.title('Input Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(true_mask_color)\n",
    "plt.title('True Mask (labelIds converted to color)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(colored_predicted_mask)\n",
    "plt.title('Predicted Mask')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation Avec Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class DataGenerator\n",
    "class DataGenerator:\n",
    "    def __init__(self, image_files, label_files, batch_size=4, image_size=(256, 256), n_classes=8, augment=False, limit=None):\n",
    "        if limit:\n",
    "            self.image_files = image_files[:limit]\n",
    "            self.label_files = label_files[:limit]\n",
    "        else:\n",
    "            self.image_files = image_files\n",
    "            self.label_files = label_files\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.n_classes = n_classes\n",
    "        self.augment = augment\n",
    "\n",
    "    def load_and_preprocess_image(self, image_path, label_path):\n",
    "        try:\n",
    "            image = img_to_array(load_img(image_path, target_size=self.image_size)) / 255.0\n",
    "            mask = np.array(load_img(label_path, target_size=self.image_size, color_mode=\"grayscale\"))\n",
    "            mask = np.eye(self.n_classes)[mask]\n",
    "            return image.astype('float32'), mask.astype('float32')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {image_path} or {label_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def augment_image(self, image, mask):\n",
    "        # Combine image and mask for consistent transformations\n",
    "        combined = tf.concat([image, mask], axis=-1)\n",
    "    \n",
    "        # Random horizontal flip\n",
    "        combined = tf.image.random_flip_left_right(combined)\n",
    "    \n",
    "        # Random vertical flip\n",
    "        combined = tf.image.random_flip_up_down(combined)\n",
    "    \n",
    "        # Random rotation using TensorFlow's built-in method\n",
    "        angles = tf.random.uniform([], minval=-0.2, maxval=0.2) \n",
    "\n",
    "        # Random zoom (using crop and resize as a replacement for zoom)\n",
    "        zoom_factor = tf.random.uniform([], minval=0.8, maxval=1.2)\n",
    "        new_height = tf.cast(self.image_size[0] * zoom_factor, tf.int32)\n",
    "        new_width = tf.cast(self.image_size[1] * zoom_factor, tf.int32)\n",
    "        combined = tf.image.resize_with_crop_or_pad(combined, new_height, new_width)\n",
    "        combined = tf.image.resize(combined, self.image_size)\n",
    "\n",
    "        # Random brightness adjustment\n",
    "        image, mask = combined[..., :3], combined[..., 3:]\n",
    "        image = tf.image.random_brightness(image, max_delta=0.05)\n",
    "    \n",
    "        # Random contrast adjustment\n",
    "        image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    \n",
    "        # Random saturation and hue adjustment\n",
    "        image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "        image = tf.image.random_hue(image, max_delta=0.05)\n",
    "    \n",
    "        return image, mask\n",
    "\n",
    "    def create_dataset(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.image_files, self.label_files))\n",
    "        dataset = dataset.map(lambda img, lbl: tf.numpy_function(self.load_and_preprocess_image, [img, lbl], [tf.float32, tf.float32]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.map(lambda img, mask: (tf.ensure_shape(img, [self.image_size[0], self.image_size[1], 3]), \n",
    "                                                 tf.ensure_shape(mask, [self.image_size[0], self.image_size[1], self.n_classes])), \n",
    "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        if self.augment:\n",
    "            dataset = dataset.map(lambda img, mask: self.augment_image(img, mask), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.batch(self.batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les chemins vers les dossiers de données prétraitées\n",
    "data_dir = 'C:/Users/DELL/Desktop/OpenClass/Formation/Projet_008/processed_data'\n",
    "\n",
    "# Charger les chemins des images et des masques pour l'entraînement et la validation\n",
    "train_image_files = [os.path.join(data_dir, 'train', 'images', f) for f in sorted(os.listdir(os.path.join(data_dir, 'train', 'images'))) if f.endswith('.png')]\n",
    "train_label_files = [os.path.join(data_dir, 'train', 'masks', f) for f in sorted(os.listdir(os.path.join(data_dir, 'train', 'masks'))) if f.endswith('.png')]\n",
    "\n",
    "val_image_files = [os.path.join(data_dir, 'val', 'images', f) for f in sorted(os.listdir(os.path.join(data_dir, 'val', 'images'))) if f.endswith('.png')]\n",
    "val_label_files = [os.path.join(data_dir, 'val', 'masks', f) for f in sorted(os.listdir(os.path.join(data_dir, 'val', 'masks'))) if f.endswith('.png')]\n",
    "\n",
    "# Définir la taille de l'ensemble de données pour les tests\n",
    "data_limit = 20 \n",
    "\n",
    "# Créer les datasets avec DataGenerator\n",
    "train_generator = DataGenerator(train_image_files, train_label_files, batch_size=2, image_size=(256, 256), n_classes=8, augment=True, limit=100).create_dataset()\n",
    "train_generator = train_generator.take(20)\n",
    "val_generator = DataGenerator(val_image_files, val_label_files, batch_size=2, image_size=(256, 256), n_classes=8, augment=False, limit=data_limit).create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer le Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)\n",
    "\n",
    "# Fonction pour calculer l'IoU corrigé\n",
    "def iou(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1, 2, 3])\n",
    "    union = K.sum(y_true, axis=[1, 2, 3]) + \\\n",
    "        K.sum(y_pred, axis=[1, 2, 3]) - intersection\n",
    "    return K.mean((intersection + 1e-6) / (union + 1e-6), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle U-Net_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_mini(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(0.5)(pool1)\n",
    "\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = Dropout(0.5)(conv3)\n",
    "\n",
    "    up4 = UpSampling2D(size=(2, 2))(pool3)\n",
    "    merge4 = concatenate([conv2, up4], axis=3)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(merge4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "\n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    merge5 = concatenate([conv1, up5], axis=3)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(merge5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(8, 1, activation='softmax')(conv5)  # 8 classes\n",
    "\n",
    "    model = Model(inputs, conv5)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss=dice_loss, metrics=[iou])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_mini(learning_rate, image_files_train, label_files_train, image_files_val, label_files_val, image_files_test, label_files_test):\n",
    "    batch_size = 4\n",
    "    print(f\"Training with learning rate: {learning_rate}\")\n",
    "    \n",
    "    train_gen = DataGenerator(image_files_train, label_files_train, batch_size=batch_size).create_dataset()\n",
    "    val_gen = DataGenerator(image_files_val, label_files_val, batch_size=batch_size).create_dataset()\n",
    "    test_gen = DataGenerator(image_files_test, label_files_test, batch_size=batch_size).create_dataset()\n",
    "\n",
    "    model = unet_mini()\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=dice_loss, metrics=[iou])\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    checkpoint_dir = f\"lr_{learning_rate}\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.keras\")\n",
    "    model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=35,\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, reduce_lr, early_stopping]  # Ajout d'EarlyStopping\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Sauvegarde du fichier history avec le nom modifié\n",
    "    with open(f'history_lr_{learning_rate}_DataAug.pkl', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "        \n",
    "    train_iou = history.history['iou'][-1]\n",
    "    val_iou = history.history['val_iou'][-1]\n",
    "    \n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    loss, test_iou = model.evaluate(test_gen, verbose=1)\n",
    "    \n",
    "    print(f\"Training Time for learning rate {learning_rate}: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Training IoU for learning rate {learning_rate}: {train_iou:.2f}\")\n",
    "    print(f\"Val IoU for learning rate {learning_rate}: {val_iou:.2f}\")\n",
    "    print(f\"Test IoU for learning rate {learning_rate}: {test_iou:.2f}\")\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save(f\"mini_unet2_LR_{learning_rate}_DataAug.keras\")\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    # clear memory\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "learning_rates = [1e-4]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    train_mini(\n",
    "        learning_rate=lr,\n",
    "        image_files_train=train_image_files,\n",
    "        label_files_train=train_label_files,\n",
    "        image_files_val=val_image_files,\n",
    "        label_files_val=val_label_files,\n",
    "        image_files_test=val_image_files,\n",
    "        label_files_test=val_label_files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle sauvegardé\n",
    "model_path = 'mini_unet2_LR_0.0001_DataAug.keras'  # Remplacez par le chemin de votre modèle\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_loss': dice_loss, 'iou': iou})\n",
    "\n",
    "def create_jet_palette(num_classes=8):\n",
    "    cmap = plt.get_cmap('jet', num_classes)\n",
    "    palette = {i: np.array(cmap(i)[:3]) * 255 for i in range(num_classes)}\n",
    "    return palette\n",
    "\n",
    "def color_mask(mask, palette):\n",
    "    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for label, color in palette.items():\n",
    "        color_mask[mask == label] = color\n",
    "    return color_mask\n",
    "\n",
    "# Créer la palette jet pour 8 classes\n",
    "palette = create_jet_palette(num_classes=8)\n",
    "\n",
    "# Choisir une image de validation spécifique\n",
    "image_index = 0  # Choisissez l'indice de l'image\n",
    "\n",
    "# Charger l'image d'entrée et le masque de référence (labelIds.png)\n",
    "input_image, true_mask = next(iter(val_generator))\n",
    "input_image = input_image.numpy()[image_index]  # Récupérer l'image en tant que NumPy array\n",
    "input_image_rescaled = (input_image * 255).astype(np.uint8)  # Rescale les valeurs pour être dans [0, 255]\n",
    "true_mask = true_mask[image_index]\n",
    "\n",
    "# Colorer le masque de référence\n",
    "true_mask_resized = np.argmax(true_mask, axis=-1)\n",
    "true_mask_color = color_mask(true_mask_resized, palette)\n",
    "\n",
    "# Effectuer la prédiction\n",
    "predicted_mask = model.predict(np.expand_dims(input_image, axis=0))[0]\n",
    "predicted_mask_resized = np.argmax(predicted_mask, axis=-1)\n",
    "\n",
    "# Colorer la prédiction pour la comparer\n",
    "colored_predicted_mask = color_mask(predicted_mask_resized, palette)\n",
    "\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(input_image_rescaled)\n",
    "plt.title('Input Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(true_mask_color)\n",
    "plt.title('True Mask (labelIds converted to color)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(colored_predicted_mask)\n",
    "plt.title('Predicted Mask')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle DeeplabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger DeepLabV3 avec un backbone ResNet50\n",
    "def deeplabv3_model(input_size=(256, 256, 3), n_classes=8):\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet', \n",
    "        include_top=False, \n",
    "        input_tensor=Input(input_size)\n",
    "    )\n",
    "    \n",
    "    # Sortie du backbone\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Remise à l'échelle de la sortie pour correspondre à la taille de l'entrée\n",
    "    x = UpSampling2D(size=(32, 32))(x) \n",
    "    \n",
    "    # Ajouter un segment de sortie personnalisé\n",
    "    x = tf.keras.layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss=dice_loss, metrics=[iou])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Modifiez votre fonction de formation pour utiliser le modèle DeepLabV3\n",
    "def train_mini(learning_rate, image_files_train, label_files_train, image_files_val, label_files_val, image_files_test, label_files_test):\n",
    "    batch_size = 4\n",
    "    print(f\"Training with learning rate: {learning_rate}\")\n",
    "    \n",
    "    train_gen = DataGenerator(image_files_train, label_files_train, batch_size=batch_size).create_dataset()\n",
    "    val_gen = DataGenerator(image_files_val, label_files_val, batch_size=batch_size).create_dataset()\n",
    "    test_gen = DataGenerator(image_files_test, label_files_test, batch_size=batch_size).create_dataset()\n",
    "\n",
    "    model = deeplabv3_model()  # Utilisation de DeepLabV3\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=dice_loss, metrics=[iou])\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    checkpoint_dir = f\"lr_{learning_rate}\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.keras\")\n",
    "    model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=35,\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, reduce_lr, early_stopping] \n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Sauvegarde du fichier history avec \"DataAug\" ajouté\n",
    "    with open(f'history_lr_{learning_rate}_DataAug.pkl', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "        \n",
    "    train_iou = history.history['iou'][-1]\n",
    "    val_iou = history.history['val_iou'][-1]\n",
    "    \n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    loss, test_iou = model.evaluate(test_gen, verbose=1)\n",
    "    \n",
    "    print(f\"Training Time for learning rate {learning_rate}: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Training IoU for learning rate {learning_rate}: {train_iou:.2f}\")\n",
    "    print(f\"Val IoU for learning rate {learning_rate}: {val_iou:.2f}\")\n",
    "    print(f\"Test IoU for learning rate {learning_rate}: {test_iou:.2f}\")\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save(f\"deeplabv3_LR_{learning_rate}_DataAug.keras\")\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    # clear memory\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "learning_rates = [1e-4]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    train_mini(\n",
    "        learning_rate=lr,\n",
    "        image_files_train=train_image_files,\n",
    "        label_files_train=train_label_files,\n",
    "        image_files_val=val_image_files,\n",
    "        label_files_val=val_label_files,\n",
    "        image_files_test=val_image_files, \n",
    "        label_files_test=val_label_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger l'historique de l'entraînement à partir du fichier pickle sauvegardé\n",
    "with open(f'history_lr_0.0001_DataAug.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "# Récupérer les données d'IoU et de Dice (Dice est calculé à partir de l'IoU)\n",
    "epochs = range(1, len(history['iou']) + 1)\n",
    "iou_train = history['iou']\n",
    "iou_val = history['val_iou']\n",
    "\n",
    "# Calculer le coefficient Dice à partir de l'IoU\n",
    "dice_train = [2 * iou / (1 + iou) for iou in iou_train]\n",
    "dice_val = [2 * iou / (1 + iou) for iou in iou_val]\n",
    "\n",
    "# Graphique d'évolution de l'IoU\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, iou_train, label='IoU Entraînement', marker='o')\n",
    "plt.plot(epochs, iou_val, label='IoU Validation', marker='o')\n",
    "plt.title('Évolution de l\\'IoU au fil des epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Graphique d'évolution du Dice coefficient\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, dice_train, label='Dice Entraînement', marker='o')\n",
    "plt.plot(epochs, dice_val, label='Dice Validation', marker='o')\n",
    "plt.title('Évolution du Dice coefficient au fil des epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model_path = 'deeplabv3_LR_0.0001_DataAug.keras'  # Remplacez par le chemin de votre modèle\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_loss': dice_loss, 'iou': iou})\n",
    "\n",
    "def create_jet_palette(num_classes=8):\n",
    "    cmap = plt.get_cmap('jet', num_classes)\n",
    "    palette = {i: np.array(cmap(i)[:3]) * 255 for i in range(num_classes)}\n",
    "    return palette\n",
    "\n",
    "def color_mask(mask, palette):\n",
    "    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for label, color in palette.items():\n",
    "        color_mask[mask == label] = color\n",
    "    return color_mask\n",
    "\n",
    "# Créer la palette jet pour 8 classes\n",
    "palette = create_jet_palette(num_classes=8)\n",
    "\n",
    "# Choisir une image de validation spécifique\n",
    "image_index = 0  # Choisissez l'indice de l'image\n",
    "\n",
    "# Charger l'image d'entrée et le masque de référence (labelIds.png)\n",
    "input_image, true_mask = next(iter(val_generator))\n",
    "input_image = input_image.numpy()[image_index]  # Récupérer l'image en tant que NumPy array\n",
    "input_image_rescaled = (input_image * 255).astype(np.uint8)  # Rescale les valeurs pour être dans [0, 255]\n",
    "true_mask = true_mask[image_index]\n",
    "\n",
    "# Colorer le masque de référence\n",
    "true_mask_resized = np.argmax(true_mask, axis=-1)\n",
    "true_mask_color = color_mask(true_mask_resized, palette)\n",
    "\n",
    "# Effectuer la prédiction\n",
    "predicted_mask = model.predict(np.expand_dims(input_image, axis=0))[0]\n",
    "predicted_mask_resized = np.argmax(predicted_mask, axis=-1)\n",
    "\n",
    "# Colorer la prédiction pour la comparer\n",
    "colored_predicted_mask = color_mask(predicted_mask_resized, palette)\n",
    "\n",
    "# Afficher les résultats\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(input_image_rescaled)\n",
    "plt.title('Input Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(true_mask_color)\n",
    "plt.title('True Mask (labelIds converted to color)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(colored_predicted_mask)\n",
    "plt.title('Predicted Mask')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
