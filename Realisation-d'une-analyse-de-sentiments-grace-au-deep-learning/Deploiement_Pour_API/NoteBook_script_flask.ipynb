{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting model load in a separate thread...\n",
      "INFO:__main__:Model loaded successfully in 0.44 seconds.\n",
      "INFO:__main__:Tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n",
      "INFO:werkzeug: * Restarting with stat\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from threading import Thread\n",
    "import logging\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Variables globales pour le modèle et un indicateur de disponibilité\n",
    "model = None\n",
    "model_ready = False\n",
    "\n",
    "# Fonction pour charger le modèle\n",
    "def load_model():\n",
    "    global model, model_ready\n",
    "    try:\n",
    "        logger.info(\"Starting model load in a separate thread...\")\n",
    "        start_time = time.time()\n",
    "        model = tf.keras.models.load_model('sentiment_lstm_model.keras')\n",
    "        model_load_time = time.time() - start_time\n",
    "        logger.info(f\"Model loaded successfully in {model_load_time:.2f} seconds.\")\n",
    "        model_ready = True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        model_ready = False\n",
    "\n",
    "# Démarrer le chargement du modèle dans un thread\n",
    "load_thread = Thread(target=load_model)\n",
    "load_thread.start()\n",
    "\n",
    "# Attendre que le modèle soit chargé\n",
    "load_thread.join()\n",
    "\n",
    "# Chargement du tokenizer\n",
    "try:\n",
    "    with open('tokenizer.json') as f:\n",
    "        data = json.load(f)\n",
    "        tokenizer = tokenizer_from_json(data)\n",
    "        logger.info(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading tokenizer: {e}\")\n",
    "\n",
    "# Prétraitement du texte\n",
    "def preprocess_text(text, max_len=60):\n",
    "    logger.info(f\"Original text: {text}\")\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    logger.info(f\"Tokenized sequence: {seq}\")\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    return padded\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if not model_ready:\n",
    "        return jsonify({'error': 'Model is not loaded yet. Please try again later.'}), 503\n",
    "\n",
    "    try:\n",
    "        data = request.get_json(force=True)\n",
    "        if 'text' not in data:\n",
    "            return jsonify({'error': 'Missing \"text\" field in request data'}), 400\n",
    "\n",
    "        logger.info(f\"Received data: {data}\")\n",
    "\n",
    "        preprocessed_text = preprocess_text(data['text'])\n",
    "        logger.info(f\"Preprocessed text: {preprocessed_text}\")\n",
    "\n",
    "        prediction_start_time = time.time()\n",
    "        prediction = model.predict(preprocessed_text)\n",
    "        prediction_time = time.time() - prediction_start_time\n",
    "        logger.info(f\"Prediction time: {prediction_time:.2f} seconds\")\n",
    "        logger.info(f\"Prediction: {prediction}\")\n",
    "\n",
    "        sentiment = \"Positif\" if prediction[0][0] > 0.5 else \"Négatif\"\n",
    "        gc.collect()\n",
    "\n",
    "        return jsonify({\n",
    "            \"score\": float(prediction[0][0]),\n",
    "            \"sentiment\": sentiment\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in predict function: {e}\")\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(debug=True)\n",
    "    except SystemExit:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513ms/step\n",
      "Tweet: I can't believe how terrible this service is. Absolutely awful!\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Tweet: Worst experience ever. Totally disappointing and frustrating.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "Tweet: Everything about this product is just so bad. Waste of money.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Tweet: I'm really not happy with how things turned out. Expected better.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Tweet: Not impressed with the quality at all. Quite disappointing.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Tweet: This is not what I ordered. Very misleading.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Tweet: I was hoping for more, but it's just okay. Not quite what I wanted.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Tweet: It's alright, but it could be a lot better. Needs improvement.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Tweet: Service was slow and unresponsive, but the product was decent.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Tweet: It’s fine, nothing too special but not too bad either.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Tweet: I had an average experience, nothing to write home about.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Tweet: It’s okay, does the job but doesn’t exceed expectations.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Tweet: Pretty good overall, just a few minor issues here and there.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Tweet: Decent product for the price, happy with the purchase.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Tweet: Met most of my expectations, would recommend with some reservations.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Tweet: Really satisfied with this. It’s just what I needed.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Tweet: Great product, would definitely buy again. Very happy.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Tweet: Good service and quality, met my expectations well.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Tweet: Absolutely love this! Exceeded all my expectations.\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Tweet: Best purchase I've made in a while. Highly recommend!\n",
      "Score: 0.2567, Sentiment: Négatif\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Tweet: Fantastic experience from start to finish. Couldn't be happier!\n",
      "Score: 0.2567, Sentiment: Négatif\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Définir le nombre maximum de mots dans le tokenizer\n",
    "max_words = 15000\n",
    "max_len = 60\n",
    "\n",
    "# Charger le modèle pré-entraîné\n",
    "model = tf.keras.models.load_model('sentiment_lstm_model.keras')\n",
    "\n",
    "# Charger et configurer le tokenizer avec le même vocabulaire que celui utilisé lors de l'entraînement\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# Supposez que tokenizer soit déjà ajusté sur le texte d'entraînement\n",
    "\n",
    "def preprocess_text(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')\n",
    "    return padded_sequence\n",
    "\n",
    "test_tweets = [\n",
    "    \"I can't believe how terrible this service is. Absolutely awful!\",\n",
    "    \"Worst experience ever. Totally disappointing and frustrating.\",\n",
    "    \"Everything about this product is just so bad. Waste of money.\",\n",
    "    \"I'm really not happy with how things turned out. Expected better.\",\n",
    "    \"Not impressed with the quality at all. Quite disappointing.\",\n",
    "    \"This is not what I ordered. Very misleading.\",\n",
    "    \"I was hoping for more, but it's just okay. Not quite what I wanted.\",\n",
    "    \"It's alright, but it could be a lot better. Needs improvement.\",\n",
    "    \"Service was slow and unresponsive, but the product was decent.\",\n",
    "    \"It’s fine, nothing too special but not too bad either.\",\n",
    "    \"I had an average experience, nothing to write home about.\",\n",
    "    \"It’s okay, does the job but doesn’t exceed expectations.\",\n",
    "    \"Pretty good overall, just a few minor issues here and there.\",\n",
    "    \"Decent product for the price, happy with the purchase.\",\n",
    "    \"Met most of my expectations, would recommend with some reservations.\",\n",
    "    \"Really satisfied with this. It’s just what I needed.\",\n",
    "    \"Great product, would definitely buy again. Very happy.\",\n",
    "    \"Good service and quality, met my expectations well.\",\n",
    "    \"Absolutely love this! Exceeded all my expectations.\",\n",
    "    \"Best purchase I've made in a while. Highly recommend!\",\n",
    "    \"Fantastic experience from start to finish. Couldn't be happier!\"\n",
    "]\n",
    "\n",
    "for tweet in test_tweets:\n",
    "    preprocessed_text = preprocess_text(tweet)\n",
    "    prediction = model.predict(preprocessed_text)\n",
    "    sentiment = \"Positif\" if prediction[0][0] > 0.5 else \"Négatif\"\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Score: {prediction[0][0]:.4f}, Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.2705337703227997, 'sentiment': 'Négatif'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://projet7oc.azurewebsites.net/predict'\n",
    "\n",
    "# Inclure à la fois 'text' et 'true_label' dans les données de la requête\n",
    "data = {\n",
    "    'text': 'Here is a sample tweet text to analyze.',\n",
    "    'true_label': '1'  # ou 'positive' ou 'negative' selon ce que le serveur attend\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
